{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShallowConvNet\n",
    "\n",
    "2024.04.18 Written by @Chahyunee (Chaehyun Lee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_dir = ''\n",
    "model_dir = ''\n",
    "class1_data, class2_data, class3_data, class4_data, class5_data= np.array, np.array, np.array, np.array, np.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_data = {}\n",
    "\n",
    "for subj_num in range(1, 6):\n",
    "    load_spec_dir = f'class{subj_num}.mat'\n",
    "    data = scipy.io.loadmat(dataset_dir + load_spec_dir)\n",
    "    data = data['data']\n",
    "    data = np.array(data)\n",
    "    data = data.reshape(600, order='F')\n",
    "    \n",
    "    # Add to dictionary\n",
    "    class_data[f'class{subj_num}_data'] = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataset class\n",
    "EEGDataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),])\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eeg_sample = (torch.tensor(self.inputs[idx], dtype=torch.float32),torch.tensor(self.labels[idx], dtype=torch.int8))\n",
    "        \n",
    "        return eeg_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Data from Dictionary and chane to PyTorch Tensor\n",
    "- Change this part!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.concatenate([class_data[f'class{i}_data'] for i in range(1, 6)], axis=0)\n",
    "\n",
    "for d in X_data:\n",
    "    d = torch.tensor(d)\n",
    "\n",
    "subj_num = 5\n",
    "y_labels = torch.tensor(np.concatenate([np.full(600, i) for i in range(5)]), dtype=torch.int64)\n",
    "y_labels_one_hot = torch.nn.functional.one_hot(y_labels, subj_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to train, validation, test\n",
    " 0.8 train, 0.2 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(X_data, y_labels_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.1 * len(dataset))\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DataLoader로 데이터 로딩\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_size, val_size, test_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ShallowNet Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "Note:\n",
    "\n",
    "    Class & def. for ShallowConvNet layer\n",
    "    - Class : LinearWithConstraint\n",
    "    - Class : Conv2dWithConstraint\n",
    "    - Def. : initialize_weight\n",
    "\n",
    "    class ShallowConvNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearWithConstraint(nn.Linear):\n",
    "    def __init__(self, *config, max_norm=1, **kwconfig):\n",
    "        self.max_norm = max_norm\n",
    "        super(LinearWithConstraint, self).__init__(*config, **kwconfig)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(LinearWithConstraint, self).forward(x)\n",
    "\n",
    "\n",
    "class Conv2dWithConstraint(nn.Conv2d):\n",
    "    def __init__(self, *config, max_norm=1, **kwconfig):\n",
    "        self.max_norm = max_norm\n",
    "        super(Conv2dWithConstraint, self).__init__(*config, **kwconfig)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data = torch.renorm(\n",
    "            self.weight.data, p=2, dim=0, maxnorm=self.max_norm\n",
    "        )\n",
    "        return super(Conv2dWithConstraint, self).forward(x)\n",
    "def initialize_weight(model, method):\n",
    "    method = dict(normal=['normal_', dict(mean=0, std=0.01)],\n",
    "                  xavier_uni=['xavier_uniform_', dict()],\n",
    "                  xavier_normal=['xavier_normal_', dict()],\n",
    "                  he_uni=['kaiming_uniform_', dict()],\n",
    "                  he_normal=['kaiming_normal_', dict()]).get(method)\n",
    "    if method is None:\n",
    "        return None\n",
    "\n",
    "    for module in model.modules():\n",
    "        # LSTM\n",
    "        if module.__class__.__name__ in ['LSTM']:\n",
    "            for param in module._all_weights[0]:\n",
    "                if param.startswith('weight'):\n",
    "                    getattr(nn.init, method[0])(getattr(module, param), **method[1])\n",
    "                elif param.startswith('bias'):\n",
    "                    nn.init.constant_(getattr(module, param), 0)\n",
    "        else:\n",
    "            if hasattr(module, \"weight\"):\n",
    "                # Not BN\n",
    "                if not (\"BatchNorm\" in module.__class__.__name__):\n",
    "                    getattr(nn.init, method[0])(module.weight, **method[1])\n",
    "                # BN\n",
    "                else:\n",
    "                    nn.init.constant_(module.weight, 1)\n",
    "                if hasattr(module, \"bias\"):\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "torch.set_printoptions(linewidth=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActSquare(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActSquare, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.square(x)\n",
    "\n",
    "\n",
    "class ActLog(nn.Module):\n",
    "    def __init__(self, eps=1e-06):\n",
    "        super(ActLog, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log(torch.clamp(x, min=self.eps))\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowConvNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    < requisiment>\n",
    "    n_classes : number of target classses\n",
    "    input_shape : s, t (e.g. [32, 1048])\n",
    "        s: number of channels\n",
    "        t: number of timepoints\n",
    "    \n",
    "    \n",
    "    < option >\n",
    "    \n",
    "    F1, F2 : filter size of the first and second layer for temporal information\n",
    "                Default: F1 = 5, F2 = 10\n",
    "    T1 : number of time points in one trial (e.g. sec x sampling rate)\n",
    "                Default: T1 = 25\n",
    "    \n",
    "    P1_T : pooling layer-temporal\n",
    "    P1_S : pooling layer-spatial\n",
    "    \n",
    "    dropout : the rate of dropout. Default: 0.5            \n",
    "    pool_mode : mode of the pooling. (mean, max) Default: 'mean'\n",
    "    \n",
    "    F1, F2  : number of temporal filters (F1) and number of pointwise\n",
    "            filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
    "    D       : number of spatial filters to learn within each temporal\n",
    "            convolution. Default: D = 2           \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            n_classes,\n",
    "            input_shape,\n",
    "            F1=5,\n",
    "            T1=25,\n",
    "            F2=10,\n",
    "            P1_T=75, # pooling layer-temporal\n",
    "            P1_S=15, # pooling layer-spatial\n",
    "            drop_out=0.5,\n",
    "            pool_mode='mean',\n",
    "            weight_init_method=None,\n",
    "            last_dim= 3072 #F2*spatial size*temporal size \n",
    "    ):\n",
    "        super(ShallowConvNet, self).__init__()\n",
    "        s, t = input_shape\n",
    "        \n",
    "        pooling_layer = dict(max=nn.MaxPool2d, mean=nn.AvgPool2d)[pool_mode]\n",
    "        \n",
    "        # chaehyunee edited ver.\n",
    "        self.constConv2d1 = Conv2dWithConstraint(1, F1, (1, T1), max_norm=2)\n",
    "        self.constConv2d2 = Conv2dWithConstraint(F1, F2, (s, 1), bias=False, max_norm=2)\n",
    "        self.bn1 = nn.BatchNorm2d(F2)\n",
    "        self.pool1 = pooling_layer((1, P1_T), (1, P1_S))\n",
    "        self.dropout1 = nn.Dropout(drop_out)\n",
    "        self.flatten1 = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(1990, last_dim) # TODO 1990 자리에 맞는 param 넣기\n",
    "        self.linear2 = nn.Linear(last_dim, n_classes)\n",
    "        self.linearConst1 = LinearWithConstraint(last_dim, n_classes, max_norm=1)\n",
    "        \n",
    "        initialize_weight(self, weight_init_method)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.constConv2d1(x)\n",
    "        x = self.constConv2d2(x)\n",
    "        x = self.bn1(x)\n",
    "        ActSquare().forward(x)\n",
    "        x = self.pool1(x)\n",
    "        ActLog().forward(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.flatten1(x)\n",
    "        x = self.linear1(x)\n",
    "        out = self.linear2(x)\n",
    "        # out = self.linearConst1(x) # TODO constraint 적용한 version으로 수정 필요\n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "def evaluate(true_labels, predicted_labels, subj_num = 5, mode='train'):\n",
    "    \n",
    "    result = dict(recall_per_class = [], f1_per_class = [], acc_per_class = [], precision_per_class = [])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        for class_idx in range(subj_num):\n",
    "            recall_class = recall_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            f1_class = f1_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            acc_class = accuracy_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            precision_class = precision_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy(), zero_division=1)\n",
    "            \n",
    "            result['recall_per_class'].append(recall_class)\n",
    "            result['f1_per_class'].append(f1_class)\n",
    "            result['acc_per_class'].append(acc_class)\n",
    "            result['precision_per_class'].append(precision_class)\n",
    "        \n",
    "    else:    \n",
    "        for class_idx in range(subj_num):\n",
    "            recall_class = recall_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            f1_class = f1_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            acc_class = accuracy_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            precision_class = precision_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy(), zero_division=1)\n",
    "            \n",
    "            result['recall_per_class'].append(recall_class)\n",
    "            result['f1_per_class'].append(f1_class)\n",
    "            result['acc_per_class'].append(acc_class)\n",
    "            result['precision_per_class'].append(precision_class)\n",
    "                    \n",
    "    result['average_recall'] = sum(result['recall_per_class']) / len(result['recall_per_class'])\n",
    "    result['average_f1'] = sum(result['f1_per_class']) / len(result['f1_per_class'])\n",
    "    result['average_acc'] = sum(result['acc_per_class']) / len(result['acc_per_class'])\n",
    "    result['average_prec'] = sum(result['precision_per_class']) / len(result['precision_per_class'])          \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change it!\n",
    "epochs = 300\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "shallow_input_shape = [32, 1024 ]\n",
    "\n",
    "model = ShallowConvNet(n_classes=5, input_shape= shallow_input_shape)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.6, min_lr=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set fixed random number seed\n",
    "seed_n = np.random.randint(500)\n",
    "print('seed is ' + str(seed_n))\n",
    "np.random.seed(seed_n)\n",
    "torch.manual_seed(seed_n)\n",
    "torch.cuda.manual_seed(seed_n)\n",
    "torch.cuda.manual_seed_all(seed_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습\n",
    "epochs = 200\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# 학습 진행 상황 데이터를 저장할 변수 설정\n",
    "history = {'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_prec': [], 'val_rec': [],\n",
    "            'train_loss': [], 'train_acc' : [], 'train_f1': [], 'train_prec': [], 'train_rec': []}\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "    \n",
    "        outputs = model(inputs.unsqueeze(1))\n",
    "        \n",
    "        # Assign 1 to the position of the largest value and 0 to the rest.\n",
    "        predicted_labels = torch.eye(outputs.shape[1])[torch.argmax(outputs.cpu(), dim=1)]\n",
    "\n",
    "        true_labels = labels.float()\n",
    "        \n",
    "        train_loss = criterion(outputs, labels.float())\n",
    "        \n",
    "        # Calculating evaluation metrics for each class in a multi-label scenario.\n",
    "        train_result = evaluate(true_labels, predicted_labels) # dictionary return\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            predicted_labels = torch.eye(outputs.shape[1])[torch.argmax(outputs.cpu(), dim=1)]\n",
    "            true_labels = labels.float()\n",
    "            \n",
    "            val_loss += criterion(outputs, true_labels)\n",
    "            \n",
    "            valid_result = evaluate(true_labels, predicted_labels, mode='valid') # dictionary return\n",
    "        \n",
    "        scheduler.step(val_loss)  \n",
    "            \n",
    "    \n",
    "\n",
    "    print(f'\\nEpoch {epoch + 1}/{epochs} \\n\\\n",
    "        train loss: {train_loss}, valid loss: {val_loss / len(val_loader)}')\n",
    "    pp(train_result)\n",
    "    pp(valid_result)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss/ len(val_loader))\n",
    "    history['train_acc'].append(train_result['average_acc'])\n",
    "    history['val_acc'].append(valid_result['average_acc'])\n",
    "    history['train_f1'].append(train_result['average_f1'])\n",
    "    history['val_f1'].append(valid_result['average_f1'])\n",
    "    history['train_prec'].append(train_result['average_prec'])\n",
    "    history['val_prec'].append(valid_result['average_prec'])\n",
    "    history['train_rec'].append(train_result['average_recall'])\n",
    "    history['val_rec'].append(valid_result['average_recall'])\n",
    "\n",
    "    \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs.unsqueeze(1)) # Feed Network\n",
    "\n",
    "        # predicted_labels = torch.round(outputs)\n",
    "        predicted_labels = torch.eye(outputs.shape[1])[torch.argmax(outputs.cpu(), dim=1)]\n",
    "        # print('predicted_labels : ', predicted_labels)\n",
    "\n",
    "        output = (torch.max(torch.exp(predicted_labels), 1)[1]).cpu().numpy()\n",
    "        print('output : ', output)\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = (torch.max(torch.exp(labels), 1)[1]).cpu().numpy()\n",
    "        print('labels  : ', labels)\n",
    "        y_true.extend(labels) # Save Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model test result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant for classes\n",
    "classes = ('class a', 'calss b', 'class c', 'class d', 'class e')\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "\n",
    "df_test = pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "\n",
    "df_test\n",
    "df_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# 현재 날짜와 시간을 문자열로 변환\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "current_time\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"YlGnBu\", vmin=0, vmax=1)\n",
    "\n",
    "# Adding labels to x-axis and y-axis\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "acc = valid_result['average_acc']\n",
    "plt.savefig(f'Figure/class{subj_num}_ShallowNet_acc{acc}_{current_time}_output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_test, annot=True, cmap=\"YlGnBu\")\n",
    "\n",
    "# Adding labels to x-axis and y-axis\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "acc = valid_result['average_acc']\n",
    "plt.savefig(f'Figure/class{subj_num}_ShallowNet_acc{acc}_{current_time}_output.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "\n",
    "val_acc = []\n",
    "history['val_acc']\n",
    "\n",
    "axes[0].plot(range(epochs), history['val_acc'], label='Valid Accuracy', color='b')\n",
    "axes[0].plot(range(epochs), history['train_acc'], label='Train Accuracy', color='r')  # Train 데이터 추가\n",
    "axes[0].set_xlabel('epochs')\n",
    "axes[0].set_ylabel('Accuracy(%)')\n",
    "axes[0].grid(linestyle='--', color='lavender')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].plot(range(epochs), [loss.item() for loss in history['val_loss']], label='Valid Loss', color='g')\n",
    "axes[1].plot(range(epochs), [loss.item() for loss in history['train_loss']], label='Train Loss', color='y')\n",
    "axes[1].set_xlabel('epochs')\n",
    "axes[1].set_ylabel('Loss(%)')\n",
    "axes[1].legend()\n",
    "axes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model save code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = f'{model_dir}ShallowNet_acc{acc}_{current_time}.pth'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model load\n",
    "model_load_path = f'{model_dir} ... .pth'\n",
    "\n",
    "loaded_model = ShallowConvNet(n_classes=5, input_shape=shallow_input_shape)\n",
    "loaded_model.load_state_dict(torch.load(model_load_path))\n",
    "loaded_model = loaded_model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
