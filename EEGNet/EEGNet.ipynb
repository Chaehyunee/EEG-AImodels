{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEGNet\n",
    "\n",
    "2024.04.18 Written by @Chahyunee (Chaehyun Lee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import datetime\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_dir = ''\n",
    "model_dir = ''\n",
    "class1_data, class2_data, class3_data, class4_data, class5_data= np.array, np.array, np.array, np.array, np.array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_data = {}\n",
    "\n",
    "for subj_num in range(1, 6):\n",
    "    load_spec_dir = f'class{subj_num}.mat'\n",
    "    data = scipy.io.loadmat(dataset_dir + load_spec_dir)\n",
    "    data = data['data']\n",
    "    data = np.array(data)\n",
    "    data = data.reshape(600, order='F')\n",
    "    print('data shape : ',data.shape)\n",
    "    \n",
    "    # Add to dictionary\n",
    "    class_data[f'class{subj_num}_data'] = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EEG Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),])\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eeg_sample = (torch.tensor(self.inputs[idx], dtype=torch.float32),torch.tensor(self.labels[idx], dtype=torch.int8))\n",
    "        \n",
    "        return eeg_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Data from Dictionary and chane to PyTorch Tensor\n",
    "- Change this part!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.concatenate([class_data[f'class{i}_data'] for i in range(1, 6)], axis=0)\n",
    "\n",
    "for d in X_data:\n",
    "    d = torch.tensor(d)\n",
    "\n",
    "subj_num = 5\n",
    "y_labels = torch.tensor(np.concatenate([np.full(600, i) for i in range(5)]), dtype=torch.int64)\n",
    "y_labels_one_hot = torch.nn.functional.one_hot(y_labels, subj_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset to train, validation, test\n",
    " 0.8 train, 0.2 validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EEGDataset(X_data, y_labels_one_hot)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = int(0.1 * len(dataset))\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # If you need, change this parameter.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEGNet Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    C : number of channels in your EEG dataset\n",
    "    T : number of time points in one trial (e.g. sec x sampling rate)\n",
    "    dropout : the rate of dropout  Default: 0.5            \n",
    "    kernelLength : size of the temporal kernel (e.g. half of T (timepoints))    \n",
    "    \n",
    "    F1, F2  : number of temporal filters (F1) and number of pointwise\n",
    "            filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
    "    D       : number of spatial filters to learn within each temporal\n",
    "            convolution. Default: D = 2        \n",
    "            \n",
    "    num_classes = number of target classses\n",
    "    \"\"\"\n",
    "    def __init__(self, C=64, T=128, dropout=0.5, kernelLength=64, F1=8, D=2, F2=16, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.channelConv = torch.nn.Conv2d(1, F1, (1, C), padding=(0, int(C/2)))\n",
    "        self.bn1 = torch.nn.BatchNorm2d(F1)\n",
    "        self.depthwiseConv1 = torch.nn.Conv2d(F1, F1, (1, kernelLength), padding=(0, int(kernelLength/2)))\n",
    "        self.pointwiseConv1 = torch.nn.Conv2d(F1, D * F1, 1)\n",
    "        \n",
    "        # Set the maximum value about model weights\n",
    "        for param in self.pointwiseConv1.parameters():\n",
    "            if param.dim() > 1:  # except to 1-dim tensor (bias)\n",
    "                param.data = torch.clamp(param.data, min=-1.0, max=1.0)\n",
    "        \n",
    "        \n",
    "        self.bn2 = torch.nn.BatchNorm2d(D*F1)\n",
    "        self.elu1 = torch.nn.ELU()\n",
    "        self.pooling1 = torch.nn.AvgPool2d((1,4))\n",
    "        # self.pooling1 = torch.nn.MaxPool2d((1,4)) # If you want to try, you can replace AvgPool with MaxPool!\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.separableConv = torch.nn.Conv2d( D * F1, D * F1, kernel_size=(1,int(kernelLength/2)), padding=(0,int(kernelLength/4)), bias=False)\n",
    "        self.pointwiseConv2 = torch.nn.Conv2d(D * F1, F2, 1, bias=False)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(F2)\n",
    "        self.elu2 = torch.nn.ELU()\n",
    "        self.pooling2 = torch.nn.AvgPool2d((1,8))\n",
    "        # self.pooling2 = torch.nn.MaxPool2d((1,8)) # If you want to try, you can replace AvgPool with MaxPool!\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(F2 * T, num_classes) \n",
    "        # self.max_norm = nn.utils.weight_norm(self.linear1, dim=None) # TODO\n",
    "        self.classifier = torch.nn.Sigmoid() if num_classes == 2 else torch.nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ##### First layer #####\n",
    "        x = self.channelConv(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwiseConv1(x)\n",
    "        x = self.pointwiseConv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.elu1(x)\n",
    "        x = self.pooling1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        \n",
    "        ##### Second layer #####\n",
    "        x = self.separableConv(x)\n",
    "        x = self.pointwiseConv2(x)\n",
    "\n",
    "        x = self.bn3(x)\n",
    "        x = self.elu2(x)\n",
    "        x = self.pooling2(x)\n",
    "        x = self.dropout2(x)\n",
    "        out = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(out)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "def evaluate(true_labels, predicted_labels, subj_num = 5, mode='train'):\n",
    "    \n",
    "    result = dict(recall_per_class = [], f1_per_class = [], acc_per_class = [], precision_per_class = [])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        for class_idx in range(subj_num):\n",
    "            recall_class = recall_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            f1_class = f1_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            acc_class = accuracy_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy())\n",
    "            precision_class = precision_score(true_labels[:, class_idx].cpu().detach().numpy(), predicted_labels[:, class_idx].cpu().detach().numpy(), zero_division=1)\n",
    "            \n",
    "            result['recall_per_class'].append(recall_class)\n",
    "            result['f1_per_class'].append(f1_class)\n",
    "            result['acc_per_class'].append(acc_class)\n",
    "            result['precision_per_class'].append(precision_class)\n",
    "        \n",
    "    else:    \n",
    "        for class_idx in range(subj_num):\n",
    "            recall_class = recall_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            f1_class = f1_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            acc_class = accuracy_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy())\n",
    "            precision_class = precision_score(true_labels[:, class_idx].cpu().numpy(), predicted_labels[:, class_idx].cpu().numpy(), zero_division=1)\n",
    "            \n",
    "            result['recall_per_class'].append(recall_class)\n",
    "            result['f1_per_class'].append(f1_class)\n",
    "            result['acc_per_class'].append(acc_class)\n",
    "            result['precision_per_class'].append(precision_class)\n",
    "                    \n",
    "    result['average_recall'] = sum(result['recall_per_class']) / len(result['recall_per_class'])\n",
    "    result['average_f1'] = sum(result['f1_per_class']) / len(result['f1_per_class'])\n",
    "    result['average_acc'] = sum(result['acc_per_class']) / len(result['acc_per_class'])\n",
    "    result['average_prec'] = sum(result['precision_per_class']) / len(result['precision_per_class'])          \n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non k-fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set fixed random number seed\n",
    "seed_n = np.random.randint(500)\n",
    "print('seed is ' + str(seed_n))\n",
    "np.random.seed(seed_n)\n",
    "torch.manual_seed(seed_n)\n",
    "torch.cuda.manual_seed(seed_n)\n",
    "torch.cuda.manual_seed_all(seed_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change it!\n",
    "epochs = 300\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "history = {'val_loss': [], 'val_acc': [], \n",
    "            'train_loss': [], 'train_acc' : []}\n",
    "\n",
    "\n",
    "model = EEGNet(num_classes=5)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "    \n",
    "        outputs, _ = model(inputs.unsqueeze(1))\n",
    "        predicted_labels = torch.round(outputs)\n",
    "        true_labels = labels.float() \n",
    "        \n",
    "        train_loss = criterion(outputs, labels.float())\n",
    "        train_losses.append(train_loss.cpu())\n",
    "        \n",
    "        train_result = evaluate(true_labels, predicted_labels) # dictionary return\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(inputs.unsqueeze(1))\n",
    "            predicted_labels = torch.round(outputs)\n",
    "            true_labels = labels.float()\n",
    "            \n",
    "            val_loss += criterion(outputs, true_labels)\n",
    "            val_losses.append(val_loss.cpu())\n",
    "            \n",
    "            valid_result = evaluate(true_labels, predicted_labels, mode='valid') # dictionary return\n",
    "            \n",
    "    \n",
    "\n",
    "    print(f'\\nEpoch {epoch + 1}/{epochs} \\n\\\n",
    "        train loss: {train_loss}, valid loss: {val_loss / len(val_loader)}')\n",
    "    pp(train_result)\n",
    "    pp(valid_result)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss / len(val_loader))\n",
    "    history['train_acc'].append(train_result['average_acc'])\n",
    "    history['val_acc'].append(valid_result['average_acc'])\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training with k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set fixed random number seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# training\n",
    "epochs = 200\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):\n",
    "        \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx) # create index\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx) \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_subsampler) \n",
    "    val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "\n",
    "    model = EEGNet(num_classes=5)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            outputs, _ = model(inputs.unsqueeze(1))\n",
    "            predicted_labels = torch.round(outputs)\n",
    "            true_labels = labels.float()\n",
    "            \n",
    "            train_loss = criterion(outputs, labels.float())\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            train_result = evaluate(true_labels, predicted_labels) # dictionary return\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            \n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs, _ = model(inputs.unsqueeze(1))\n",
    "                predicted_labels = torch.round(outputs)\n",
    "                true_labels = labels.float()\n",
    "                \n",
    "                val_loss += criterion(outputs, true_labels)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                valid_result = evaluate(true_labels, predicted_labels, mode='valid') # dictionary return\n",
    "                \n",
    "        \n",
    "    \n",
    "        print(f'\\n{fold+1} fold & Epoch {epoch + 1}/{epochs} \\n\\\n",
    "            train loss: {train_loss}, valid loss: {val_loss / len(val_loader)}')\n",
    "        pp(train_result)\n",
    "        pp(valid_result)\n",
    "    \n",
    "    \n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, _ = model(inputs.unsqueeze(1)) # Feed Network\n",
    "\n",
    "        predicted_labels = torch.round(outputs)\n",
    "        print('predicted_labels : ', predicted_labels)\n",
    "\n",
    "        output = (torch.max(torch.exp(predicted_labels), 1)[1]).cpu().numpy()\n",
    "        print('output : ', output)\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = (torch.max(torch.exp(labels), 1)[1]).cpu().numpy()\n",
    "        print('labels  : ', labels)\n",
    "        y_true.extend(labels) # Save Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant for classes\n",
    "classes = ('class a', 'calss b', 'class c', 'class d', 'class e')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "\n",
    "df_test = pd.DataFrame(cf_matrix)\n",
    "\n",
    "df_test\n",
    "df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True, cmap=\"YlGnBu\")\n",
    "\n",
    "# Adding labels to x-axis and y-axis\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "acc = valid_result['average_acc']\n",
    "plt.savefig(f'Figure/class{subj_num}_EEGNet_acc{acc}_{current_time}_output.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model save code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "model_save_path = f'{model_dir}EEGNet_acc{acc}_{current_time}.pth'\n",
    "model_save_path\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Load code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = EEGNet(num_classes=5)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "loaded_model = loaded_model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
